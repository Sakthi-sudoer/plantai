import os

from langchain.chains import LLMChain
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain_core.messages import SystemMessage
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain_groq import ChatGroq



"""
This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.
"""

# Get Groq API key
groq_api_key = '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'
model = 'llama3-8b-8192'
# Initialize Groq Langchain chat object and conversation
groq_chat = ChatGroq(
        groq_api_key=groq_api_key, 
        model_name=model
)

system_prompt = 'You are a friendly plant living in the users home garden in urban area, asking your needs based on moisture = 70percent , light = good , temp = ok , leaves and health analysis by image status = healthy use no bold texts but lot of emojis and short answers'
conversational_memory_length = 10 # number of previous messages the chatbot will remember during the conversation

memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key="chat_history", return_messages=True)

def response(user_input):
    user_question = user_input



    # Construct a chat prompt template using various components
    prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content=system_prompt
            ),  # This is the persistent system prompt that is always included at the start of the chat.

            MessagesPlaceholder(
                variable_name="chat_history"
            ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.

            HumanMessagePromptTemplate.from_template(
                "{human_input}"
            ),  # This template is where the user's current input will be injected into the prompt.
        ]
    )

    # Create a conversation chain using the LangChain LLM (Language Learning Model)
    conversation = LLMChain(
        llm=groq_chat,  # The Groq LangChain chat object initialized earlier.
        prompt=prompt,  # The constructed prompt template.
        verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.
        memory=memory,  # The conversational memory object that stores and manages the conversation history.
    )
    # The chatbot's answer is generated by sending the full prompt to the Groq API.
    response = conversation.predict(human_input=user_question)
    return response

if __name__ == "__main__":
    main()